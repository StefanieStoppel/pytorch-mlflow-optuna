{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training a PyTorch MNIST model with MlFlow and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "import mlflow\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from mlflow import pytorch\n",
    "from pprint import pformat\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_set_size = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            # Log batch loss using mlflow\n",
    "            mlflow.log_metric(\"train_batch_loss\", loss.item())\n",
    "            batch_size = len(data)\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * batch_size}/{train_set_size} \"\n",
    "                  f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_set_size = len(test_loader.dataset)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= test_set_size\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{test_set_size} \"\n",
    "          f\"({100. * correct / test_set_size:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artifact_path(active_run):\n",
    "    parsed_uri = urlparse(active_run.info.artifact_uri)\n",
    "    artifact_path = os.path.abspath(os.path.join(parsed_uri.netloc, parsed_uri.path))\n",
    "    return artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.8, step=0.1)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 256, step=4)\n",
    "    optimizer = trial.suggest_categorical(\"optim\", [\"Adam\", \"Adadelta\"])\n",
    "    print(f\"Trial parameters: {pformat([trial.params])}\")\n",
    "    return lr, dropout, optimizer, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def objective(trial, experiment, params=None):\n",
    "    # Start mlflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "        active_run = mlflow.active_run()\n",
    "        artifact_path = get_artifact_path(active_run)\n",
    "        print(f\"Artifact path for this run: {artifact_path}\")\n",
    "        \n",
    "        # Use mlflow to log params\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Use CUDA if GPU is available\n",
    "        use_cuda = params[\"use_cuda\"] and torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        # Log mlflow device parameter\n",
    "        mlflow.log_param(\"device\", device)\n",
    "\n",
    "        torch.manual_seed(params[\"seed\"])\n",
    "\n",
    "        # Use hyperparameter suggestions created by Optuna\n",
    "        lr, dropout, optimizer_name, batch_size = suggest_hyperparameters(trial)\n",
    "        \n",
    "        # Load the MNIST train and test dataset and save it to ./data\n",
    "        mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                       ]))\n",
    "        train_loader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True)\n",
    "        mnist_test = datasets.MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                       ]))\n",
    "        test_loader = torch.utils.data.DataLoader(mnist_test,\n",
    "                                                  batch_size=params[\"validation_batch_size\"],\n",
    "                                                  shuffle=True)\n",
    "\n",
    "        model = Net().to(device)\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        if optimizer_name == \"Adadelta\":\n",
    "            optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=params[\"gamma\"])\n",
    "\n",
    "        for epoch in range(0, params[\"epochs\"]):\n",
    "            train(params, model, device, train_loader, optimizer, epoch)\n",
    "            test(model, device, test_loader)\n",
    "            scheduler.step()\n",
    "    \n",
    "        if params[\"save_model\"]:\n",
    "            pytorch.save_model(model, f\"{artifact_path}/mnist_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def main(params=None):\n",
    "    # create mlflow experiment if it doesn't exist already\n",
    "    experiment_name = params[\"experiment_name\"]\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Create the optuna study whih has the same name as the experiment\n",
    "    study = optuna.create_study(study_name=experiment_name, direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, experiment, params), n_trials=5)\n",
    "\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact path for this run: /home/steffi/dev/blog/pytorch-mlflow-optuna/mlruns/0/4578e1800fb0494aaaa90f25bca103f6/artifacts\n",
      "Trial parameters: [{'batch_size': 232,\n",
      "  'dropout': 0.2,\n",
      "  'lr': 0.007334813642845111,\n",
      "  'optim': 'Adadelta'}]\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.321307\n",
      "Train Epoch: 0 [2320/60000 (0%)]\tLoss: 2.289355\n",
      "Train Epoch: 0 [4640/60000 (0%)]\tLoss: 2.264903\n",
      "Train Epoch: 0 [6960/60000 (0%)]\tLoss: 2.245275\n",
      "Train Epoch: 0 [9280/60000 (0%)]\tLoss: 2.207915\n",
      "Train Epoch: 0 [11600/60000 (0%)]\tLoss: 2.179073\n",
      "Train Epoch: 0 [13920/60000 (0%)]\tLoss: 2.141765\n",
      "Train Epoch: 0 [16240/60000 (0%)]\tLoss: 2.066733\n",
      "Train Epoch: 0 [18560/60000 (0%)]\tLoss: 1.987784\n",
      "Train Epoch: 0 [20880/60000 (0%)]\tLoss: 1.902032\n",
      "Train Epoch: 0 [23200/60000 (0%)]\tLoss: 1.850267\n",
      "Train Epoch: 0 [25520/60000 (0%)]\tLoss: 1.718433\n",
      "Train Epoch: 0 [27840/60000 (0%)]\tLoss: 1.658704\n",
      "Train Epoch: 0 [30160/60000 (0%)]\tLoss: 1.583560\n",
      "Train Epoch: 0 [32480/60000 (0%)]\tLoss: 1.431161\n",
      "Train Epoch: 0 [34800/60000 (0%)]\tLoss: 1.285771\n",
      "Train Epoch: 0 [37120/60000 (0%)]\tLoss: 1.199129\n",
      "Train Epoch: 0 [39440/60000 (0%)]\tLoss: 1.169201\n",
      "Train Epoch: 0 [41760/60000 (0%)]\tLoss: 1.148735\n",
      "Train Epoch: 0 [44080/60000 (0%)]\tLoss: 1.069772\n",
      "Train Epoch: 0 [46400/60000 (0%)]\tLoss: 0.983309\n",
      "Train Epoch: 0 [48720/60000 (0%)]\tLoss: 0.905490\n",
      "Train Epoch: 0 [51040/60000 (0%)]\tLoss: 0.889222\n",
      "Train Epoch: 0 [53360/60000 (0%)]\tLoss: 0.821034\n",
      "Train Epoch: 0 [55680/60000 (0%)]\tLoss: 0.800906\n",
      "Train Epoch: 0 [58000/60000 (0%)]\tLoss: 0.655803\n",
      "Test set: Average loss: 0.5984, Accuracy: 8574/10000 (86%)\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.785057\n",
      "Train Epoch: 1 [2320/60000 (0%)]\tLoss: 0.756071\n",
      "Train Epoch: 1 [4640/60000 (0%)]\tLoss: 0.731706\n",
      "Train Epoch: 1 [6960/60000 (0%)]\tLoss: 0.734053\n",
      "Train Epoch: 1 [9280/60000 (0%)]\tLoss: 0.737773\n",
      "Train Epoch: 1 [11600/60000 (0%)]\tLoss: 0.709195\n",
      "Train Epoch: 1 [13920/60000 (0%)]\tLoss: 0.706705\n",
      "Train Epoch: 1 [16240/60000 (0%)]\tLoss: 0.727289\n",
      "Train Epoch: 1 [18560/60000 (0%)]\tLoss: 0.758322\n",
      "Train Epoch: 1 [20880/60000 (0%)]\tLoss: 0.638777\n",
      "Train Epoch: 1 [23200/60000 (0%)]\tLoss: 0.706316\n",
      "Train Epoch: 1 [25520/60000 (0%)]\tLoss: 0.777459\n",
      "Train Epoch: 1 [27840/60000 (0%)]\tLoss: 0.622901\n",
      "Train Epoch: 1 [30160/60000 (0%)]\tLoss: 0.641220\n",
      "Train Epoch: 1 [32480/60000 (0%)]\tLoss: 0.552955\n",
      "Train Epoch: 1 [34800/60000 (0%)]\tLoss: 0.631711\n",
      "Train Epoch: 1 [37120/60000 (0%)]\tLoss: 0.707326\n",
      "Train Epoch: 1 [39440/60000 (0%)]\tLoss: 0.570371\n",
      "Train Epoch: 1 [41760/60000 (0%)]\tLoss: 0.614257\n",
      "Train Epoch: 1 [44080/60000 (0%)]\tLoss: 0.661648\n",
      "Train Epoch: 1 [46400/60000 (0%)]\tLoss: 0.576284\n",
      "Train Epoch: 1 [48720/60000 (0%)]\tLoss: 0.549113\n",
      "Train Epoch: 1 [51040/60000 (0%)]\tLoss: 0.546980\n",
      "Train Epoch: 1 [53360/60000 (0%)]\tLoss: 0.617619\n",
      "Train Epoch: 1 [55680/60000 (0%)]\tLoss: 0.612692\n",
      "Train Epoch: 1 [58000/60000 (0%)]\tLoss: 0.582123\n",
      "Test set: Average loss: 0.3996, Accuracy: 8947/10000 (89%)\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.625979\n",
      "Train Epoch: 2 [2320/60000 (0%)]\tLoss: 0.547977\n",
      "Train Epoch: 2 [4640/60000 (0%)]\tLoss: 0.561915\n",
      "Train Epoch: 2 [6960/60000 (0%)]\tLoss: 0.577888\n",
      "Train Epoch: 2 [9280/60000 (0%)]\tLoss: 0.576460\n",
      "Train Epoch: 2 [11600/60000 (0%)]\tLoss: 0.653959\n",
      "Train Epoch: 2 [13920/60000 (0%)]\tLoss: 0.510899\n",
      "Train Epoch: 2 [16240/60000 (0%)]\tLoss: 0.565939\n",
      "Train Epoch: 2 [18560/60000 (0%)]\tLoss: 0.567117\n",
      "Train Epoch: 2 [20880/60000 (0%)]\tLoss: 0.455025\n"
     ]
    }
   ],
   "source": [
    "PARAMS = {\n",
    "    \"experiment_name\": \"pytorch-optuna-mlflow\",\n",
    "    \"batch_size\": 128,\n",
    "    \"validation_batch_size\": 1000,\n",
    "    \"epochs\": 3,\n",
    "    \"gamma\": 0.7,\n",
    "    \"use_cuda\": False,\n",
    "    \"seed\": 42,\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True\n",
    "}\n",
    "main(params=PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}